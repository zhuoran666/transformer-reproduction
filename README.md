# 手工搭建用于机器翻译的Transformer模型

本项目是一个从零开始实现的Transformer模型，其架构基于开创性论文《Attention Is All You Need》[1]。整个模型完全使用PyTorch的基础模块搭建，没有调用官方高级API `nn.Transformer`，旨在深入探究该架构的核心组件。

项目的主要任务是**德语到英语（DE-EN）的机器翻译**，并在 **IWSLT2017 数据集** 上进行训练和评估。整个项目的结构便于进行系统性的实验，包括一系列的消融研究，以分析不同架构选择和超参数对模型性能的影响。

## 功能特性

本实现包含了一套完整的功能，覆盖了模型架构、先进的训练技巧以及一个健壮的实验框架。

### 核心模型架构（手工实现）
- **完整的编码器-解码器架构**: 从零实现了编码器和解码器堆栈。
- **多头自注意力机制**: 手工实现了带有多个并行头的缩放点积注意力机制。
- **逐位置前馈网络**: 标准的前馈网络（FFN）子层。
- **残差连接与层归一化**: 正确实现了“Add & Norm”模块，这对于训练深度模型至关重要。
- **可配置的位置编码**:
    - 经典的 sinusoidal（绝对）位置编码。
    - T5风格的相对位置编码（RPE）作为备选方案。

### 训练优化技巧
- **AdamW 优化器**: 利用解耦的权重衰减进行更好的正则化。
- **自定义学习率调度器**: 实现了Transformer论文中带有预热（warmup）阶段的特定学习率调度策略。
- **梯度裁剪**: 防止梯度爆炸，稳定训练过程。
- **自动混合精度 (AMP)**: 使用FP16在现代GPU上加速训练。
- **`torch.compile()` 支持**: 集成了PyTorch 2.0+的即时（JIT）编译功能以获得进一步的速度提升。
- **Xavier 初始化**: 改善模型训练初期的稳定性。
- **标签平滑**: 一种关键的正则化技巧，用于防止模型过拟合。

### 实验框架
- **集中的 `run.sh` 脚本**: 通过单一命令轻松运行基线和各种消融实验。
- **全面的日志记录**: 所有训练进度和结果都保存在结构化的目录和日志文件中。
- **自动化训练曲线可视化**: 为每个实验自动生成并保存训练/验证损失曲线图。
- **动态模型加载**: 评估脚本能根据保存的超参数，智能地加载不同架构的模型。

## 项目结构

```
.
|-- data/
|   `-- iwslt17_de_en_local/    # 用于存放本地IWSLT2017数据集文件的目录
|-- runs/
|   `-- <实验名称>/               # 每个实验的输出（日志、模型、图表）都保存在这里
|-- src/
|   |-- dataset.py            # 数据加载、预处理和DataLoader创建
|   |-- model.py              # 手工实现的Transformer模型代码
|   |-- train.py              # 主要的训练和验证脚本
|   `-- evaluate.py           # 用于在测试集上评估已训练模型的脚本
|-- report.pdf                # 最终的项目报告 (由report.tex编译)
|-- report.tex                # 报告的LaTeX源码
`-- run.sh                    # 用于运行所有实验的主脚本
```

## 环境设置与安装

1.  **克隆本仓库**
    ```bash
    git clone <your-repo-url>
    cd <your-repo-name>
    ```

2.  **创建 Conda 环境**
    ```bash
    conda create -n transformer python=3.10
    conda activate transformer
    ```

3.  **安装依赖**
    ```bash
    pip install torch torchtext==0.17.1 spacy tqdm matplotlib
    ```

4.  **下载 SpaCy 模型**
    ```bash
    python -m spacy download de_core_news_sm
    python -m spacy download en_core_web_sm
    ```

5.  **准备数据集**
    请手动下载 IWSLT2017 DE-EN 数据集，并将其文件放入 `data/iwslt17_de_en_local/` 目录下。代码已配置为读取纯文本格式的 `train.tags.*` 文件进行训练，以及XML格式的 `*.xml` 文件进行验证和测试。

## 如何运行

所有实验均通过 `run.sh` 脚本管理。

1.  **为脚本添加执行权限:**
    ```bash
    chmod +x run.sh
    ```

2.  **运行一个实验:**
    该脚本接受一个实验名称作为参数。所有结果将保存在 `runs/<实验名称>` 目录下。

    *   **运行新的、性能强大的基线模型（3层模型）:**
        ```bash
        ./run.sh new_baseline
        ```

    *   **运行一个特定的消融实验，例如，移除位置编码:**
        ```bash
        ./run.sh ablation-no-pe
        ```

    *   **运行挑战性实验（带有强正则化的深度模型）:**
        ```bash
        ./run.sh challenge-deep-regularized
        ```

    *   **按顺序运行所有已定义的实验:**
        ```bash
        ./run.sh all
        ```

## 实验结果总结

通过系统性的消融研究，我们发现对于IWSLT2017数据集，一个**更小、更浅的模型**其性能显著优于标准的6层“Transformer Base”架构。在该数据集上的主要挑战并非模型容量不足，而是深度模型带来的**优化困难和严重的过拟合**问题。

我们性能最佳的模型（`new_baseline`）是一个**3层的Transformer**，其 `d_model=512`。

### 核心性能指标

| 实验名称                   | 配置                                | BLEU  | 核心洞见                                                 |
| ------------------------------ | ----------------------------------- | :---: | -------------------------------------------------------- |
| **新基线 (New Baseline)**        | **3层, d_model=512**                | **14.78** | **性能最强，平衡性好。**                                 |
| 原始基线 (作参考)        | 6层, d_model=512                    | 1.65  | 存在严重的过拟合和优化问题。                             |
| 挑战：深度正则化模型 | 6层 + 强正则化                    | 0.52  | 强正则化未能拯救深度模型。                               |
| 消融: 更窄的模型           | 3层, d_model=256                    | 14.28 | 性能略低，但速度更快、效率更高。                         |
| 消融: 移除位置编码       | 3层, 无PE                           | 7.11  | 性能显著下降，但模型仍学到了一定的对齐能力。             |
| 消融: 相对位置编码 (RPE)   | 3层, 使用RPE                        | 13.77 | 在此任务上，性能不如更简单的绝对位置编码。               |
| 消融: 更大Dropout          | 3层, Dropout=0.3                    | 13.22 | 对于此模型尺寸，过度正则化反而损害了性能。               |


### 关键洞见总结
1.  **深度是主要问题**: 对于此数据集规模，将模型深度从3层增加到6层会因优化挑战而急剧损害性能，这是一个强正则化也无法解决的问题。
2.  **位置编码至关重要，但...**: 移除显式的位置编码虽然非常有害，但模型并未完全失效（仍有7.11 BLEU）。这表明，深度架构本身（特别是残差连接和层归一化的堆叠）似乎隐式地保留了一些微弱的序列信息。
3.  **正则化是一场平衡艺术**: 对于一个尺寸已经合适的模型（如3层基线），施加过度的正则化（如高Dropout率）会导致欠拟合，从而降低性能。

## 参考文献
[1] Ashish Vaswani, et al. "Attention Is All You Need." _Advances in Neural Information Processing Systems (NIPS)_, 2017.
